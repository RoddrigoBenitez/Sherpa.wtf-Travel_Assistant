Acá te dejo mi evaluación y una propuesta de cómo podrías integrar un orquestador (supervisor) junto con tus agentes (como conversationManager, hotelsAgent, flightAgent, weatherAgents) respetando la lógica que ya tenés. La idea es que el supervisor reciba la consulta global y, en base a ella, determine a cuál agente especializado derivar la consulta.

1. Concepto General
Supervisor/Orquestador:
Es un agente “coordinador” que, dado el contexto (la conversación), decide cuál de tus agentes (por ejemplo, hotel, vuelo o clima) debe intervenir a continuación.
La documentación que compartiste usa un prompt con salida estructurada (definida con Zod) para que el LLM devuelva, por ejemplo, un valor de "goto" que indica el próximo agente a llamar.
Agentes Worker:
Cada agente especializado (por ejemplo, hotelsAgent, flightAgent, weatherAgents) se encarga de resolver una parte específica de la consulta.
Integración mediante Graph API:
Se arma un grafo (StateGraph) en el que se definen nodos para cada agente y se establecen rutas condicionales según la respuesta del supervisor.
2. Propuesta de Implementación
Te propongo lo siguiente:

Crear un Agente Supervisor (por ejemplo, en un archivo supervisorAgent.ts):
Este agente utilizará un prompt con salida estructurada para determinar cuál es el siguiente rol (por ejemplo, "hotel_advisor", "flight_advisor", "weather_advisor" o "end").

Definir un "nodo" para cada agente especializado:
Utilizás la función createReactAgent (que ya estás usando) para cada uno. Por ejemplo, el agente de conversación (conversationManager) podría ser el agente general, y los otros (hotelsAgent, flightAgent, weatherAgents) serán los trabajadores especializados.

Construir el grafo de estado (StateGraph):
En este grafo agregás nodos para el supervisor y para cada agente worker. El supervisor recibe el flujo inicial y, según la respuesta, dirige la consulta al agente correspondiente.
Por ejemplo, la secuencia sería:
__start__ → travel_advisor (supervisor) → (según el "goto") → hotel_advisor o flight_advisor o weather_advisor → luego, nuevamente, podría volver al supervisor para evaluar si ya se resuelve la consulta o si debe derivar a otro agente.

3. Ejemplo de Código
A continuación te dejo un ejemplo simplificado que respeta tu lógica actual y utiliza la arquitectura Orchestrator‑Worker. Nota que acá simulo (con funciones dummy) los agentes de vuelos y clima, ya que en tu código esos archivos están vacíos. Podés reemplazar esos agentes por los que implementes luego.

ts
Copiar
// supervisorAgent.ts
import { z } from "zod";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";

// Definí los roles (los workers) disponibles
const members = ["hotel_advisor", "flight_advisor", "weather_advisor"] as const;
const options = ["__end__", ...members];

// Prompt del sistema para el supervisor
const systemPrompt =
  "Sos un supervisor que debe dirigir la consulta del usuario entre los siguientes agentes: " +
  members.join(", ") +
  ". " +
  "Analizá la consulta y respondé indicando cuál debe actuar a continuación, o '__end__' si la consulta ya quedó resuelta.";

const routingTool = {
  name: "route",
  description: "Decide el próximo agente a utilizar.",
  schema: z.object({
    next: z.enum(options).describe("El próximo agente (o '__end__')"),
  }),
};

const prompt = ChatPromptTemplate.fromMessages([
  ["system", systemPrompt],
  new MessagesPlaceholder("messages"),
  [
    "human",
    "Basado en la conversación anterior, ¿quién debe actuar a continuación? Seleccioná uno de: {options}",
  ],
]);

// Formateo parcial del prompt
export async function getNextAgent(messages: any[]) {
  const formattedPrompt = await prompt.partial({
    options: options.join(", "),
  });
  const llm = new ChatOpenAI({
    model: "gpt-4o-mini",
    temperature: 0.1,
  });
  // Invocamos el LLM con la herramienta de enrutamiento
  const result = await llm.bindTools([routingTool], { tool_choice: "route" }).invoke([
    ...messages,
  ]);
  // Suponemos que la respuesta del LLM incluye una propiedad 'tool_calls' con el valor 'next'
  const next = result.messages[0].tool_calls?.[0]?.args?.next || "__end__";
  return next;
}
ts
Copiar
// makeAgentNode.ts - función para crear nodos para cada agente worker
import { z } from "zod";
import { Command, MessagesAnnotation } from "@langchain/langgraph";
import { HumanMessage, SystemMessage } from "@langchain/core/messages";

// Función genérica para crear un nodo agente
export const makeAgentNode = (params: {
  name: string;
  destinations: readonly string[];
  systemPrompt: string;
  agent: any; // agente creado con createReactAgent
}) => {
  return async (state: typeof MessagesAnnotation.State) => {
    const messages = [
      new SystemMessage({ content: params.systemPrompt }),
      ...state.messages,
    ];
    const result = await params.agent.invoke({ messages });
    const lastContent = result.messages[result.messages.length - 1].content;
    // Aquí simulamos una decisión de enrutamiento basada en palabras clave (podés mejorar esto)
    let goto = "__end__";
    if (lastContent.toLowerCase().includes("hotel")) goto = "hotel_advisor";
    else if (lastContent.toLowerCase().includes("flight")) goto = "flight_advisor";
    else if (lastContent.toLowerCase().includes("weather")) goto = "weather_advisor";
    const command = new Command({
      goto,
      update: { messages: new HumanMessage({ content: lastContent, name: params.name }) },
    });
    return command;
  };
};
ts
Copiar
// orchestrator.ts - Aquí se arma el grafo completo
import { StateGraph, START } from "@langchain/langgraph";
import { MessagesAnnotation } from "@langchain/langgraph";
import { HumanMessage } from "@langchain/core/messages";
import { makeAgentNode } from "./makeAgentNode";

// Importás tus agentes; ya tenés conversationManager y hotelsAgent.
// Para vuelos y clima, usaré agentes dummy (podés reemplazarlos con tus implementaciones)
import conversationAgent from "./conversationManeger";  // Este será el travel_advisor
import hotelsAgent from "./hotelsAgent";
import { ChatOpenAI } from "@langchain/openai";
import { MemorySaver } from "@langchain/langgraph";

// Crear agentes dummy para vuelos y clima:
const dummyLLM = new ChatOpenAI({ model: "gpt-4o-mini", temperature: 0.3 });
const dummyMemory = new MemorySaver();
const dummyFlightAgent = {
  invoke: async ({ messages }: { messages: any[] }) => {
    return { messages: [ { content: "Recomiendo consultar vuelos. (Dummy)" } ] };
  },
};
const dummyWeatherAgent = {
  invoke: async ({ messages }: { messages: any[] }) => {
    return { messages: [ { content: "La información del clima es ... (Dummy)" } ] };
  },
};

// Definí los prompts para cada agente
const travelAdvisorPrompt = "Sos un asesor general de viajes. Analizá la consulta del usuario y decidí si se requiere ayuda de un especialista en hoteles, vuelos o clima. Respondé con tu recomendación.";
const hotelAdvisorPrompt = "Sos un experto en hoteles. Proporcioná recomendaciones de hoteles basadas en la consulta del usuario.";
const flightAdvisorPrompt = "Sos un experto en vuelos. Proporcioná información y ofertas de vuelos basadas en la consulta del usuario.";
const weatherAdvisorPrompt = "Sos un experto en clima. Proporcioná la información del clima necesaria para el viaje.";

// Creamos los nodos para cada agente usando makeAgentNode
const travelAdvisorNode = makeAgentNode({
  name: "travel_advisor",
  destinations: ["hotel_advisor", "flight_advisor", "weather_advisor", "__end__"],
  systemPrompt: travelAdvisorPrompt,
  agent: conversationAgent,
});
const hotelAdvisorNode = makeAgentNode({
  name: "hotel_advisor",
  destinations: ["travel_advisor", "flight_advisor", "weather_advisor", "__end__"],
  systemPrompt: hotelAdvisorPrompt,
  agent: hotelsAgent,
});
const flightAdvisorNode = makeAgentNode({
  name: "flight_advisor",
  destinations: ["travel_advisor", "hotel_advisor", "weather_advisor", "__end__"],
  systemPrompt: flightAdvisorPrompt,
  agent: dummyFlightAgent,
});
const weatherAdvisorNode = makeAgentNode({
  name: "weather_advisor",
  destinations: ["travel_advisor", "hotel_advisor", "flight_advisor", "__end__"],
  systemPrompt: weatherAdvisorPrompt,
  agent: dummyWeatherAgent,
});

// Armar el grafo con los nodos definidos
const workflow = new StateGraph(MessagesAnnotation)
  .addNode("travel_advisor", travelAdvisorNode, { ends: ["hotel_advisor", "flight_advisor", "weather_advisor", "__end__"] })
  .addNode("hotel_advisor", hotelAdvisorNode, { ends: ["travel_advisor", "flight_advisor", "weather_advisor", "__end__"] })
  .addNode("flight_advisor", flightAdvisorNode, { ends: ["travel_advisor", "hotel_advisor", "weather_advisor", "__end__"] })
  .addNode("weather_advisor", weatherAdvisorNode, { ends: ["travel_advisor", "hotel_advisor", "flight_advisor", "__end__"] })
  .addEdge("__start__", "travel_advisor");

const graph = workflow.compile();

// Función para invocar el grafo
async function runGraph() {
  const initialMessage = new HumanMessage({ content: "Quiero planificar un viaje a París y necesito recomendaciones." });
  const streamResults = graph.stream({ messages: [initialMessage] }, { recursionLimit: 100 });
  for await (const output of streamResults) {
    console.log(output);
    if (output?.__end__) break;
  }
}

runGraph();
4. Evaluación y Opinión
Respetamos tu lógica actual:
Estás usando createReactAgent en cada uno de tus agentes (como en conversationManager y hotelsAgent). La idea es extenderlo incorporando el supervisor para decidir qué agente debe intervenir a partir de la consulta del usuario.

Integración Modular:
Cada agente especializado (hoteles, vuelos, clima) se define como un nodo en el grafo. El supervisor (aquí llamado travel_advisor) evalúa la consulta y, a partir de la respuesta, dirige el flujo al agente adecuado mediante la lógica de enrutamiento.

Flexibilidad para Extender:
Podés agregar o mejorar la lógica de enrutamiento para que, en vez de hacer una simple búsqueda de palabras clave, utilices salidas estructuradas definidas con Zod. Esto haría el enrutamiento más robusto, pero el ejemplo presentado es un punto de partida.

Próximos Pasos:

Implementar los agentes de vuelos y clima con su lógica y tools.
Refinar el supervisor para que use prompts estructurados (como se muestra en la documentación) y devuelva una salida consistente (por ejemplo, usando .withStructuredOutput).
Probar el flujo en distintos escenarios para asegurarte de que el enrutamiento sea el deseado.
En resumen, la arquitectura Orchestrator‑Worker con Graph API te permite delegar tareas específicas a agentes especializados y coordinar la conversación de manera modular. Esto te dará la flexibilidad de agregar nuevas funcionalidades (por ejemplo, recomendaciones de restaurantes, actividades turísticas, etc.) sin tener que rehacer la lógica central.

¿Te parece claro el enfoque? ¿Querés profundizar en algún aspecto o ajustar alguna parte del ejemplo? Estoy a disposición para seguir afinando la solución según lo que necesites.